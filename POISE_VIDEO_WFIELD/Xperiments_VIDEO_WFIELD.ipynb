{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "spoken-republic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from gibbs_sampler_poise.ipynb\n",
      "importing Jupyter notebook from kl_divergence_calculator.ipynb\n",
      "importing Jupyter notebook from data_preprocessing.ipynb\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import import_ipynb\n",
    "import gibbs_sampler_poise\n",
    "import kl_divergence_calculator\n",
    "import data_preprocessing\n",
    "from torchvision.utils import save_image\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.nn import functional as F  #for the activation function\n",
    "from torchviz import make_dot\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision\n",
    "import umap\n",
    "import random\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "another-hands",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning parameters\n",
    "latent_dim1 = 32\n",
    "latent_dim2 = 16\n",
    "batch_size = 10\n",
    "dim_VIDEO   = 19200\n",
    "dim_WFIELD   = 2* 135* 160\n",
    "lr = 1e-4\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tx = transforms.ToTensor()\n",
    "VIDEO_TEST_PATH     = \"/hdd/achint_files/musall_behavior/video_test_data.npy\"\n",
    "\n",
    "WFIELD_TEST_PATH     = \"/hdd/achint_files/wfield_data/DOWNSAMPLED_wfield_test_data.npy\"\n",
    "PATH = \"/home/achint/Practice_code/Synthetic_dataset/POISE_VIDEO_WFIELD/untitled.txt\"\n",
    "SUMMARY_WRITER_PATH = \"/home/achint/Practice_code/logs\"\n",
    "CROSS_RECONSTRUCTION_PATH = \"/home/achint/Practice_code/Synthetic_dataset/POISE_VIDEO_WFIELD/reconstructions_experiments/cross_generation/\"\n",
    "JOINT_RECONSTRUCTION_PATH = \"/home/achint/Practice_code/Synthetic_dataset/POISE_VIDEO_WFIELD/reconstructions_experiments/joint_generation/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "elegant-child",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing VIDEO and WFIELD datasets\n",
    "joint_dataset_test  = data_preprocessing.EvilMouDataSet(video_dir=VIDEO_TEST_PATH,\n",
    "                                                        wfield_dir = WFIELD_TEST_PATH)\n",
    "\n",
    "joint_dataset_test_loader = DataLoader(\n",
    "    joint_dataset_test,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "skilled-pleasure",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self,latent_dim1, latent_dim2, batch_size,use_mse_loss=True):\n",
    "        super(VAE,self).__init__()\n",
    "        self.latent_dim1 = latent_dim1\n",
    "        self.latent_dim2 = latent_dim2\n",
    "        self.batch_size = batch_size\n",
    "        self.use_mse_loss = use_mse_loss\n",
    "        self.gibbs                   = gibbs_sampler_poise.gibbs_sampler(self.latent_dim1, self.latent_dim2, self.batch_size)  \n",
    "        self.kl_div                  = kl_divergence_calculator.kl_divergence(self.latent_dim1, self.latent_dim2, self.batch_size)\n",
    "        ## Encoder set1(VIDEO)\n",
    "        self.set1_enc1 = nn.Linear(in_features = dim_VIDEO,out_features = 512)\n",
    "        self.set1_enc2 = nn.Linear(in_features = 512,out_features = 128)\n",
    "        self.set1_enc3 = nn.Linear(in_features = 128,out_features = 2*latent_dim1) \n",
    "        ## Decoder set1(VIDEO)\n",
    "        self.set1_dec1 = nn.Linear(in_features = latent_dim1,out_features = 128)\n",
    "        self.set1_dec2 = nn.Linear(in_features = 128,out_features = 512)\n",
    "        self.set1_dec3 = nn.Linear(in_features = 512,out_features = dim_VIDEO)\n",
    "        ## Encoder set2(WFIELD)\n",
    "        # input size: 1x2 x 135 x 160\n",
    "        self.set2_enc1 = nn.Conv2d(in_channels=2, out_channels=latent_dim2, kernel_size=4, stride=2, padding=1)\n",
    "        # size: 1x16 x 67 x 80\n",
    "        self.set2_enc2 = nn.Conv2d(in_channels=latent_dim2, out_channels=latent_dim2, kernel_size=4, stride=2, padding=1)\n",
    "        # size: 1x16 x 33 x 40\n",
    "        self.set2_enc3 = nn.Conv2d(in_channels=latent_dim2, out_channels=latent_dim2, kernel_size=10, stride=(6,8), padding=1)\n",
    "        # size: 16 x 5 x 5   \n",
    "        ## Decoder set2(WFIELD)\n",
    "        # input size: 16x1x1\n",
    "        self.set2_dec0 = nn.ConvTranspose2d(in_channels=latent_dim2,out_channels=latent_dim2, kernel_size=5, stride=2, padding=0)\n",
    "        # input size: 16x5x5\n",
    "        self.set2_dec1 = nn.ConvTranspose2d(in_channels=latent_dim2,out_channels=latent_dim2, kernel_size=10, stride=(6,8), padding=1,output_padding=(1,0))\n",
    "        # size: 16 x 33 x 40\n",
    "        self.set2_dec2 = nn.ConvTranspose2d(in_channels=latent_dim2,out_channels=2, kernel_size=6, stride=4, padding=1,output_padding=(3,0))\n",
    "        # size: 16 x 135 x 160\n",
    "        \n",
    "        self.VIDEOc1 = nn.Conv2d(latent_dim2, latent_dim2, 4, 2, 0)\n",
    "        # size: 16 x 1 x 1\n",
    "        self.VIDEOc2 = nn.Conv2d(latent_dim2, latent_dim2, 4, 2, 0)\n",
    "        # size: 16 x 1 x 1\n",
    "        self.register_parameter(name='g11', param = nn.Parameter(torch.randn(latent_dim1,latent_dim2)))\n",
    "        self.register_parameter(name='g22', param = nn.Parameter(torch.randn(latent_dim1,latent_dim2)))\n",
    "        self.flag_initialize= 1\n",
    "        self.g12= torch.zeros(latent_dim1,latent_dim2).to(device)\n",
    "    def forward(self,x1,x2):\n",
    "        data1    = x1 #VIDEO\n",
    "        data2    = x2 #WFIELD\n",
    "        # Modality 1 (VIDEO)\n",
    "        x1       = F.relu(self.set1_enc1(x1))\n",
    "        x1       = F.relu(self.set1_enc2(x1))  \n",
    "        x1       = self.set1_enc3(x1).view(-1,2,latent_dim1)  # ->[128,2,32]\n",
    "        mu1      = x1[:,0,:] # ->[128,32]\n",
    "        log_var1 = x1[:,1,:] # ->[128,32]\n",
    "        var1     = -torch.exp(log_var1)           #lambdap_1<0\n",
    "        # Modality 2 (WFIELD)\n",
    "        x2 = x2.view(-1,2, 135,160) \n",
    "        x2 = F.relu(self.set2_enc1(x2))\n",
    "        x2 = F.relu(self.set2_enc2(x2))\n",
    "        x2 = F.relu(self.set2_enc3(x2))\n",
    "        mu2 = (self.VIDEOc1(x2).squeeze(3)).squeeze(2)\n",
    "        log_var2 = (self.VIDEOc2(x2).squeeze(3)).squeeze(2)\n",
    "        var2     = -torch.exp(log_var2)           #lambdap_2<0     \n",
    "        g22      = -torch.exp(self.g22) \n",
    "\n",
    "# Initializing gibbs sample        \n",
    "        if self.flag_initialize==1:\n",
    "            z1_prior,z2_prior = self.gibbs.gibbs_sample(self.flag_initialize,\n",
    "                                                        torch.zeros_like(mu1),\n",
    "                                                        torch.zeros_like(mu2),\n",
    "                                                        self.g11,\n",
    "                                                        g22,\n",
    "                                                        torch.zeros_like(mu1),\n",
    "                                                        torch.zeros_like(var1),\n",
    "                                                        torch.zeros_like(mu2),\n",
    "                                                        torch.zeros_like(var2),\n",
    "                                                        n_iterations=5000)\n",
    "            z1_posterior,z2_posterior = self.gibbs.gibbs_sample(self.flag_initialize,\n",
    "                                                                torch.zeros_like(mu1),\n",
    "                                                                torch.zeros_like(mu2),\n",
    "                                                                self.g11,\n",
    "                                                                g22,\n",
    "                                                                mu1, \n",
    "                                                                var1,\n",
    "                                                                mu2,\n",
    "                                                                var2,\n",
    "                                                                n_iterations=5000)\n",
    "            self.z1_prior        = z1_prior\n",
    "            self.z2_prior        = z2_prior\n",
    "            self.z1_posterior    = z1_posterior\n",
    "            self.z2_posterior    = z2_posterior\n",
    "            self.flag_initialize = 0\n",
    "        z1_prior     = self.z1_prior.detach()\n",
    "        z2_prior     = self.z2_prior.detach()\n",
    "        z1_posterior = self.z1_posterior.detach()\n",
    "        z2_posterior = self.z2_posterior.detach()\n",
    "        self.z1_gibbs_prior,self.z2_gibbs_prior         = self.gibbs.gibbs_sample(self.flag_initialize,\n",
    "                                                                                  z1_prior,\n",
    "                                                                                  z2_prior,\n",
    "                                                                                  self.g11,\n",
    "                                                                                  g22,\n",
    "                                                                                  torch.zeros_like(mu1),\n",
    "                                                                                  torch.zeros_like(var1),\n",
    "                                                                                  torch.zeros_like(mu2),\n",
    "                                                                                  torch.zeros_like(var2),\n",
    "                                                                                  n_iterations=5)\n",
    "        self.z1_gibbs_posterior,self.z2_gibbs_posterior = self.gibbs.gibbs_sample(self.flag_initialize,\n",
    "                                                                                  z1_posterior,\n",
    "                                                                                  z2_posterior,\n",
    "                                                                                  self.g11,\n",
    "                                                                                  g22,\n",
    "                                                                                  mu1,\n",
    "                                                                                  var1,\n",
    "                                                                                  mu2,\n",
    "                                                                                  var2,\n",
    "                                                                                  n_iterations=5)\n",
    "        self.z1_posterior = self.z1_gibbs_posterior.detach()\n",
    "        self.z2_posterior = self.z2_gibbs_posterior.detach()\n",
    "        self.z1_prior = self.z1_gibbs_prior.detach()\n",
    "        self.z2_prior = self.z2_gibbs_prior.detach()\n",
    "        G1 = torch.cat((self.g11,self.g12),0)\n",
    "        G2 = torch.cat((self.g12,g22),0)\n",
    "        G  = torch.cat((G1,G2),1)\n",
    "        self.z2_gibbs_posterior = self.z2_gibbs_posterior.unsqueeze(2)\n",
    "        self.z2_gibbs_posterior = self.z2_gibbs_posterior.unsqueeze(3)\n",
    "        # decoding for VIDEO\n",
    "        x1 = F.relu(self.set1_dec1(self.z1_gibbs_posterior))\n",
    "        x1 = self.set1_dec2(x1)\n",
    "        # decoding for WFIELD\n",
    "        x2 = F.relu(self.set2_dec0(self.z2_gibbs_posterior))\n",
    "        x2 = F.relu(self.set2_dec1(x2))\n",
    "        if self.use_mse_loss:\n",
    "            reconstruction1 = self.set1_dec3(x1)\n",
    "            reconstruction2 = self.set2_dec2(x2).view(-1,dim_WFIELD)\n",
    "        else:\n",
    "            reconstruction1 = torch.sigmoid(self.set1_dec3(x1))\n",
    "            reconstruction2 = torch.sigmoid(self.set2_dec2(x2))\n",
    "\n",
    "        self.z2_gibbs_posterior = self.z2_gibbs_posterior.squeeze()\n",
    "#         self.z2_gibbs_posterior = self.z2_gibbs_posterior.unsqueeze(0)\n",
    "        # calculating loss\n",
    "        part_fun0,part_fun1,part_fun2 = self.kl_div.calc(G,self.z1_gibbs_posterior,self.z2_gibbs_posterior,self.z1_gibbs_prior,self.z2_gibbs_prior,mu1,var1,mu2,var2)\n",
    "        if self.use_mse_loss:\n",
    "            mse_loss = nn.MSELoss(reduction='sum')\n",
    "            MSE1 = mse_loss(reconstruction1, data1)\n",
    "            MSE2 = mse_loss(reconstruction2, data2)\n",
    "\n",
    "        else:\n",
    "            bce_loss = nn.BCELoss(reduction='sum')\n",
    "            MSE1 = bce_loss(reconstruction1, data1)\n",
    "            MSE2 = bce_loss(reconstruction2, data2)\n",
    "\n",
    "        KLD  = part_fun0+part_fun1+part_fun2\n",
    "        loss = MSE1+MSE2+KLD\n",
    "        return self.z1_posterior,self.z2_posterior,reconstruction1,reconstruction2,mu1,var1,mu2,var2,loss, MSE1, MSE2, KLD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "developed-three",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g11\n",
      "g22\n",
      "set1_enc1.weight\n",
      "set1_enc1.bias\n",
      "set1_enc2.weight\n",
      "set1_enc2.bias\n",
      "set1_enc3.weight\n",
      "set1_enc3.bias\n",
      "set1_dec1.weight\n",
      "set1_dec1.bias\n",
      "set1_dec2.weight\n",
      "set1_dec2.bias\n",
      "set1_dec3.weight\n",
      "set1_dec3.bias\n",
      "set2_enc1.weight\n",
      "set2_enc1.bias\n",
      "set2_enc2.weight\n",
      "set2_enc2.bias\n",
      "set2_enc3.weight\n",
      "set2_enc3.bias\n",
      "set2_dec0.weight\n",
      "set2_dec0.bias\n",
      "set2_dec1.weight\n",
      "set2_dec1.bias\n",
      "set2_dec2.weight\n",
      "set2_dec2.bias\n",
      "VIDEOc1.weight\n",
      "VIDEOc1.bias\n",
      "VIDEOc2.weight\n",
      "VIDEOc2.bias\n"
     ]
    }
   ],
   "source": [
    "state = torch.load(PATH)\n",
    "model = VAE(latent_dim1, latent_dim2, batch_size,use_mse_loss=True).to(device)\n",
    "optimizer = optim.Adam(model.parameters(),lr=lr)\n",
    "model.load_state_dict(state['state_dict'])\n",
    "optimizer.load_state_dict(state['optimizer'])\n",
    "for name, para in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "taken-formation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model,joint_dataloader,epoch,cross_generation,joint_generation):\n",
    "\n",
    "    model.eval()\n",
    "    latent_repVIDEO  = []\n",
    "    latent_repWFIELD = []\n",
    "    running_mse1 = 0.0\n",
    "    running_mse2 = 0.0\n",
    "    running_kld  = 0.0\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i,joint_data in enumerate(joint_dataloader):\n",
    "            data1   = joint_data[0]\n",
    "            data1   = data1.float()\n",
    "            data2   = joint_data[1]\n",
    "            data2   = data2.float()\n",
    "            data1   = data1.to(device)\n",
    "            data2   = data2.to(device)\n",
    "            data1   = data1.view(data1.size(0), -1)\n",
    "            data2   = data2.view(data2.size(0), -1)\n",
    "            if cross_generation:\n",
    "                data2   = torch.zeros_like(data2)\n",
    "            elif joint_generation:\n",
    "                data1   = torch.zeros_like(data1)\n",
    "                data2   = torch.zeros_like(data2)\n",
    "            z1_posterior,z2_posterior,reconstruction1,reconstruction2,mu1,var1,mu2,var2,loss, MSE1, MSE2, KLD = model(data1,data2)  \n",
    "            latent_repVIDEO.extend(z1_posterior.cpu().numpy())\n",
    "            latent_repWFIELD.extend(z2_posterior.cpu().numpy())\n",
    "            running_loss += loss.item()\n",
    "            running_mse1 += MSE1.item()\n",
    "            running_mse2 += MSE2.item()\n",
    "            running_kld += KLD.item()\n",
    "            #save the last batch input and output of every epoch\n",
    "            if i == int(len(joint_dataloader.dataset)/joint_dataloader.batch_size) - 1:\n",
    "                num_rows = 8\n",
    "                trans = transforms.Compose([transforms.Resize((135,160))])\n",
    "                both  = torch.cat((data1.view(batch_size, 1, 160, 120)[:8], \n",
    "                                  reconstruction1.view(batch_size, 1, 160, 120)[:8]))\n",
    "\n",
    "                bothp = torch.cat((data2.view(batch_size, 2, 135, 160)[:8], \n",
    "                                  reconstruction2.view(batch_size, 2, 135, 160)[:8]))\n",
    "\n",
    "                both = torch.cat((both,both),axis=1)\n",
    "\n",
    "                both = trans(both)\n",
    "\n",
    "                both_single = torch.cat((both,bothp),0)\n",
    "                both_single = torch.cat((both,bothp),0)\n",
    "\n",
    "                if cross_generation: \n",
    "                    save_image(both_single.cpu(), os.path.join(CROSS_RECONSTRUCTION_PATH, f\"cross_generation_{epoch}.png\"), nrow=num_rows)\n",
    "                elif joint_generation:\n",
    "                    save_image(both_single.cpu(), os.path.join(JOINT_RECONSTRUCTION_PATH, f\"joint_generation_{epoch}.png\"), nrow=num_rows)\n",
    "    test_loss = running_loss/(len(joint_dataloader.dataset))\n",
    "    mse1_loss = running_mse1 / (len(joint_dataloader.dataset))\n",
    "    mse2_loss = running_mse2 / (len(joint_dataloader.dataset))\n",
    "    kld_loss = running_kld / (len(joint_dataloader.dataset))\n",
    "    writer.add_scalar(\"validation/loss\", test_loss, epoch)\n",
    "    writer.add_scalar(\"validation/MSE1\", mse1_loss, epoch)\n",
    "    writer.add_scalar(\"validation/MSE2\", mse2_loss, epoch)\n",
    "    writer.add_scalar(\"validation/KLD\", kld_loss, epoch)\n",
    "    return test_loss,latent_repVIDEO,latent_repWFIELD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "hungarian-tract",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 1\n",
      "Test Loss: 5404.1009\n"
     ]
    }
   ],
   "source": [
    "test_loss = []\n",
    "epochs = 1\n",
    "writer=SummaryWriter(SUMMARY_WRITER_PATH)\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1} of {epochs}\")\n",
    "    test_epoch_loss,latent_repVIDEO,latent_repWFIELD = test(model,\n",
    "                                                           joint_dataset_test_loader,\n",
    "                                                           epoch,\n",
    "                                                           cross_generation=True,\n",
    "                                                           joint_generation=False)\n",
    "    test_loss.append(test_epoch_loss)     \n",
    "    print(f\"Test Loss: {test_epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "independent-calendar",
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = np.asarray(latent_repVIDEO)\n",
    "z2 = np.asarray(latent_repWFIELD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "specialized-brooklyn",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7820, 16)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "placed-passage",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "achint-env2",
   "language": "python",
   "name": "achint-env2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
