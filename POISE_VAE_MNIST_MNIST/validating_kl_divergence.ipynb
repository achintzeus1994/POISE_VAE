{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cordless-broad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from kl_divergence_calculator.ipynb\n",
      "importing Jupyter notebook from gibbs_sampler_poise.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "import kl_divergence_calculator\n",
    "import gibbs_sampler_poise\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "organic-concentration",
   "metadata": {},
   "outputs": [],
   "source": [
    "gibbs                   = gibbs_sampler_poise.gibbs_sampler()  \n",
    "kl_div                  = kl_divergence_calculator.kl_divergence()\n",
    "latent_dim1             = 1\n",
    "latent_dim2             = 1\n",
    "batch_size              = 1\n",
    "device                  = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "later-tribe",
   "metadata": {},
   "source": [
    "The KL divergence is given by,\n",
    "<span class=\"math display\">\n",
    "    \\begin{aligned}\n",
    "    D_{KL}(q(\\mathbf{z, z'|x, x'}) \\Vert p(\\mathbf{z, z'})) &= \\mathrm{log}\\left[\\frac{\\tilde{q}_{\\phi}(z^{(i)},z'^{(i)}|x,x')}{\\tilde{p}_{\\theta}(z^{(i)},z'^{(i)})}\\right]-\\langle \\mathbf{sg}( \\mathbb{E}_q[T]),\\lambda\\rangle-\\langle \\mathbf{sg}( \\mathbb{E}_q[T']),\\lambda'\\rangle+(\\mathbb{E}_p-\\mathbb{E}_q)\\langle \\mathbf{sg}[T\\otimes T'],G\\rangle\n",
    "    \\end{aligned}\n",
    "</span>\n",
    "\n",
    "<span class=\"math display\">\n",
    "    \\begin{aligned}\n",
    "    T_{prior}&=[z_{prior},z^2_{prior}]\\\\\n",
    "    T'_{prior}&=[z'_{prior},z'^2_{prior}]\\\\\n",
    "    T_{posterior}&=[z_{posterior},z^2_{posterior}]\\\\\n",
    "    T'_{posterior}&=[z'_{posterior},z'^2_{posterior}]\\\\\n",
    "    \\lambda&=[\\lambda_1,\\lambda_2]\\\\\n",
    "    \\lambda'&=[\\lambda'_1,\\lambda'_2]\\\\\n",
    "    T_{prior}^2&=(z^2_{prior}+z'^2_{prior})\\\\\n",
    "    T_{posterior}^2&=(z^2_{posterior}+z'^2_{posterior})\n",
    "    \\end{aligned}\n",
    "</span>\n",
    "We also define,\n",
    "<span class=\"math display\">\n",
    "    \\begin{aligned}\n",
    "    \\mathrm{log}p_{prior}&=-T_{posterior}^2+T_{posterior}*G*T'_{posterior}\\\\\n",
    "    \\mathrm{log}p_{posterior}&=-T_{posterior}^2+T_{posterior}*G*T'_{posterior}+\\lambda*T_{posterior}+\\lambda'*T'_{posterior}\\\\\n",
    "    \\end{aligned}\n",
    "</span>\n",
    "The three terms of the partition function are given by:\n",
    "<span class=\"math display\">\n",
    "    \\begin{aligned}\n",
    "    part_0&=\\sum \\mathrm{log}p_{posterior}-\\mathrm{log}p_{prior}=\\langle \\lambda,T_{posterior}\\rangle +\\langle \\lambda',T'_{posterior}\\rangle \\\\\n",
    "    part_1&=- \\langle \\lambda,sgd(T_{posterior})\\rangle -\\langle  \\lambda',sgd(T'_{posterior})\\rangle \\\\\n",
    "    part_2&=(\\mathbb{E}_p-\\mathbb{E}_q)\\langle \\mathbf{sg}[T\\otimes T'],G\\rangle=\\mathbb{E}_p\\langle \\mathbf{sg}[T_{prior}\\otimes T'_{prior}],G\\rangle-\\mathbb{E}_q\\langle \\mathbf{sg}[T_{posterior}\\otimes T'_{posterior}],G\\rangle\n",
    "    \\end{aligned}\n",
    "</span>\n",
    "The KL divergence is given by,\n",
    "<span class=\"math display\">\n",
    "    \\begin{aligned}\n",
    "    D_{KL}(q(\\mathbf{z, z'|x, x'}) \\Vert p(\\mathbf{z, z'})&=\\langle \\lambda,T_{posterior}\\rangle +\\langle \\lambda',T'_{posterior}\\rangle- \\langle \\lambda,sgd(T_{posterior})\\rangle -\\langle  \\lambda',sgd(T'_{posterior})\\rangle +(\\mathbb{E}_p-\\mathbb{E}_q)\\langle \\mathbf{sg}[T\\otimes T'],G\\rangle\n",
    "    \\end{aligned}\n",
    "</span>\n",
    "For G=0, we have,\n",
    "<span class=\"math display\">\n",
    "    \\begin{aligned}\n",
    "    D_{KL}(q(\\mathbf{z, z'|x, x'}) \\Vert p(\\mathbf{z, z'}))&=\\langle \\lambda,T_{posterior}\\rangle +\\langle \\lambda',T'_{posterior}\\rangle- \\langle \\lambda,sgd(T_{posterior})\\rangle -\\langle  \\lambda',sgd(T'_{posterior})\\rangle \n",
    "    \\end{aligned}\n",
    "</span>\n",
    "The gradients are:\n",
    "<span class=\"math display\">\n",
    "    \\begin{aligned}\n",
    "    \\frac{\\partial D_{KL}}{\\partial \\lambda} &= T_{posterior}+\\lambda\\frac{\\partial T_{posterior}}{\\partial \\lambda} -  sgd(T_{posterior})=\\lambda\\frac{\\partial T_{posterior}}{\\partial \\lambda} \\\\\n",
    "    \\frac{\\partial D_{KL}}{\\partial \\lambda'} &= T'_{posterior}+\\lambda'\\frac{\\partial T'_{posterior}}{\\partial \\lambda'} -  sgd(T'_{posterior})=\\lambda'\\frac{\\partial T'_{posterior}}{\\partial \\lambda'} \\\\\n",
    "    \\frac{\\partial D_{KL}}{\\partial T_{posterior}} &=\\lambda\\\\\n",
    "    \\frac{\\partial D_{KL}}{\\partial T'_{posterior}} &=\\lambda'\\\\\n",
    "    \\end{aligned}\n",
    "</span>\n",
    "For independent Gaussians,\n",
    "<span class=\"math display\">\n",
    "    \\begin{aligned}\n",
    "     D_{KL}&=\\frac{1}{2}[\\mu^2+\\sigma^2-\\mathrm{log}(\\sigma^2)-1]+\\frac{1}{2}[\\mu'^2+\\sigma'^2-\\mathrm{log}(\\sigma'^2)-1]\n",
    "    \\end{aligned}\n",
    "</span>\n",
    "The gradients are:\n",
    "<span class=\"math display\">\n",
    "    \\begin{aligned}\n",
    "     \\frac{\\partial D_{KL}}{\\partial \\mu}&=\\mu\n",
    "    \\end{aligned}\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "unlikely-municipality",
   "metadata": {},
   "outputs": [],
   "source": [
    "g11  = torch.zeros(latent_dim1,latent_dim2).to(device)\n",
    "g22  = torch.zeros(latent_dim1,latent_dim2).to(device)        \n",
    "g12  = torch.zeros(latent_dim1,latent_dim2).to(device)\n",
    "G1   = torch.cat((g11,g12),0).to(device)\n",
    "G2   = torch.cat((g12,g22),0).to(device)\n",
    "G    = torch.cat((G1,G2),1)\n",
    "mu1  = torch.randn(latent_dim1,latent_dim2).to(device)\n",
    "var1 = torch.randn(latent_dim1,latent_dim2).to(device)\n",
    "mu2  = torch.randn(latent_dim1,latent_dim2).to(device)\n",
    "var2 = torch.randn(latent_dim1,latent_dim2).to(device)\n",
    "\n",
    "z1_prior,z2_prior = gibbs.initialize_prior_sample(g11,g22)\n",
    "z1_posterior,z2_posterior = gibbs.initialize_posterior_sample(g11,g22,mu1, var1,mu2,var2)\n",
    "                                                                                \n",
    "\n",
    "part_fun0,part_fun1,part_fun2 = kl_div.calc(G,z1_posterior,z2_posterior,z1_prior,z2_prior,mu1,var1,mu2,var2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "auburn-adapter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "part_fun0\n",
    "part_fun1\n",
    "part_fun2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "anonymous-raleigh",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-9efd343c2dd8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpart_fun0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/achint-env2/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/achint-env2/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "x=part_fun0.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "retained-neutral",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.4754, device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "part_fun0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chief-nickname",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "achint-env2",
   "language": "python",
   "name": "achint-env2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
