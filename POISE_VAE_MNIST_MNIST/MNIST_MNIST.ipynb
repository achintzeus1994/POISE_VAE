{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "temporal-bullet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from data_preprocessing.ipynb\n",
      "importing Jupyter notebook from gibbs_sampler_poise.ipynb\n",
      "importing Jupyter notebook from kl_divergence_calculator.ipynb\n"
     ]
    }
   ],
   "source": [
    "## Importing libraries\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import argparse\n",
    "import matplotlib\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "import import_ipynb\n",
    "import data_preprocessing\n",
    "import gibbs_sampler_poise\n",
    "import kl_divergence_calculator\n",
    "#import config\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import copy\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "matplotlib.style.use('ggplot')\n",
    "from torch.nn import functional as F  #for the activation function\n",
    "import umap\n",
    "from torchviz import make_dot\n",
    "import shutil\n",
    "#random.seed(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "premium-reasoning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning parameters\n",
    "latent_dim1 = 1\n",
    "latent_dim2 = 1\n",
    "batch_size = 1\n",
    "dim_MNIST   = 784\n",
    "lr = 1e-4\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device=torch.device('cpu')\n",
    "tx = transforms.ToTensor()\n",
    "MNIST_TRAINING_PATH = \"/home/achint/Practice_code/VAE/MNIST/MNIST/processed/training.pt\"\n",
    "MNIST_TEST_PATH     = \"/home/achint/Practice_code/VAE/MNIST/MNIST/processed/test.pt\"\n",
    "SUMMARY_WRITER_PATH = \"/home/achint/Practice_code/logs\"\n",
    "RECONSTRUCTION_PATH = \"/home/achint/Practice_code/Synthetic_dataset/POISE_VAE_MNIST_MNIST/reconstructions/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fifty-portuguese",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the logs directory and the reconstruction directory \n",
    "if os.path.exists(RECONSTRUCTION_PATH):\n",
    "    shutil.rmtree(RECONSTRUCTION_PATH)\n",
    "    os.makedirs(RECONSTRUCTION_PATH)\n",
    "\n",
    "if os.path.exists(SUMMARY_WRITER_PATH):\n",
    "    shutil.rmtree(SUMMARY_WRITER_PATH)\n",
    "    os.makedirs(SUMMARY_WRITER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "configured-laser",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing MNIST and MNIST datasets\n",
    "joint_dataset_train = data_preprocessing.JointDataset(mnist_pt_path_1 = MNIST_TRAINING_PATH,\n",
    "                                                      mnist_pt_path_2 = MNIST_TRAINING_PATH)\n",
    "joint_dataset_test = data_preprocessing.JointDataset(mnist_pt_path_1 = MNIST_TEST_PATH,\n",
    "                                                     mnist_pt_path_2 = MNIST_TEST_PATH)\n",
    "\n",
    "joint_dataset_train_loader = DataLoader(\n",
    "    joint_dataset_train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "joint_dataset_test_loader = DataLoader(\n",
    "    joint_dataset_test,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "controlled-fiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self,latent_dim1, latent_dim2, batch_size,use_mse_loss=True):\n",
    "        super(VAE,self).__init__()\n",
    "        self.latent_dim1 = latent_dim1\n",
    "        self.latent_dim2 = latent_dim2\n",
    "        self.batch_size = batch_size\n",
    "        self.use_mse_loss = use_mse_loss\n",
    "        self.gibbs                   = gibbs_sampler_poise.gibbs_sampler(self.latent_dim1, self.latent_dim2, self.batch_size)  \n",
    "        self.kl_div                  = kl_divergence_calculator.kl_divergence(self.latent_dim1, self.latent_dim2, self.batch_size)\n",
    "        ## Encoder set1(MNIST)\n",
    "        self.set1_enc1 = nn.Linear(in_features = dim_MNIST,out_features = 512)\n",
    "        self.set1_enc2 = nn.Linear(in_features = 512,out_features = 128)\n",
    "        self.set1_enc3 = nn.Linear(in_features = 128,out_features = 2*latent_dim1)\n",
    "        ## Encoder set2(MNIST)\n",
    "        self.set2_enc1 = nn.Linear(in_features = dim_MNIST,out_features = 512)\n",
    "        self.set2_enc2 = nn.Linear(in_features = 512,out_features = 128)\n",
    "        self.set2_enc3 = nn.Linear(in_features = 128,out_features = 2*latent_dim2) \n",
    "        ## Decoder set1(MNIST)\n",
    "        self.set1_dec1 = nn.Linear(in_features = latent_dim1,out_features = 128)\n",
    "        self.set1_dec2 = nn.Linear(in_features = 128,out_features = 512)\n",
    "        self.set1_dec3 = nn.Linear(in_features = 512,out_features = dim_MNIST)\n",
    "        ## Decoder set2(MNIST)\n",
    "        self.set2_dec1 = nn.Linear(in_features = latent_dim2,out_features = 128)\n",
    "        self.set2_dec2 = nn.Linear(in_features = 128,out_features = 512)\n",
    "        self.set2_dec3 = nn.Linear(in_features = 512,out_features = dim_MNIST)\n",
    "#         self.register_parameter(name='g11', param = nn.Parameter(torch.randn(latent_dim1,latent_dim2)))\n",
    "#         self.register_parameter(name='g22', param = nn.Parameter(torch.randn(latent_dim1,latent_dim2)))\n",
    "        self.flag_initialize= 1\n",
    "        self.g11=torch.zeros(latent_dim1,latent_dim2).to(device)\n",
    "        self.g22=torch.zeros(latent_dim1,latent_dim2).to(device)        \n",
    "        self.g12= torch.zeros(latent_dim1,latent_dim2).to(device)\n",
    "    def forward(self,x1,x2):\n",
    "        data1    = x1 #MNIST\n",
    "        data2    = x2 #MNIST\n",
    "\n",
    "        # Modality 1 (MNIST)\n",
    "        x1       = F.relu(self.set1_enc1(x1))\n",
    "        x1       = F.relu(self.set1_enc2(x1))  \n",
    "        x1       = self.set1_enc3(x1).view(-1,2,latent_dim1)  # ->[128,2,32]\n",
    "        mu1      = x1[:,0,:] # ->[128,32]\n",
    "        log_var1 = x1[:,1,:] # ->[128,32]\n",
    "        var1     = -torch.exp(log_var1)           #lambdap_2<0\n",
    "        # Modality 2 (MNIST)\n",
    "        x2       = F.relu(self.set2_enc1(x2))\n",
    "        x2       = F.relu(self.set2_enc2(x2))  \n",
    "        x2       = self.set2_enc3(x2).view(-1,2,latent_dim2)  # ->[128,2,32]\n",
    "        mu2      = x2[:,0,:] # ->[128,32]\n",
    "        log_var2 = x2[:,1,:] # ->[128,32]\n",
    "        var2     = -torch.exp(log_var2)           #lambdap_2<0     \n",
    "#       g22      = -torch.exp(self.g22) \n",
    "        g22      = self.g22    \n",
    "\n",
    "\n",
    "# Initializing gibbs sample        \n",
    "        if self.flag_initialize==1:\n",
    "            z1_prior,z2_prior = self.gibbs.gibbs_sample(self.flag_initialize,\n",
    "                                                        torch.zeros_like(mu1),\n",
    "                                                        torch.zeros_like(mu2),\n",
    "                                                        self.g11,\n",
    "                                                        g22,\n",
    "                                                        torch.zeros_like(mu1),\n",
    "                                                        torch.zeros_like(var1),\n",
    "                                                        torch.zeros_like(mu2),\n",
    "                                                        torch.zeros_like(var2),\n",
    "                                                        n_iterations=5000)\n",
    "            z1_posterior,z2_posterior = self.gibbs.gibbs_sample(self.flag_initialize,\n",
    "                                                                torch.zeros_like(mu1),\n",
    "                                                                torch.zeros_like(mu2),\n",
    "                                                                self.g11,\n",
    "                                                                g22,\n",
    "                                                                mu1, \n",
    "                                                                var1,\n",
    "                                                                mu2,\n",
    "                                                                var2,\n",
    "                                                                n_iterations=5000)\n",
    "            self.z1_prior        = z1_prior\n",
    "            self.z2_prior        = z2_prior\n",
    "            self.z1_posterior    = z1_posterior\n",
    "            self.z2_posterior    = z2_posterior\n",
    "            self.flag_initialize = 0\n",
    "        z1_prior     = self.z1_prior.detach()\n",
    "        z2_prior     = self.z2_prior.detach()\n",
    "        z1_posterior = self.z1_posterior.detach()\n",
    "        z2_posterior = self.z2_posterior.detach()\n",
    "        self.z1_gibbs_prior,self.z2_gibbs_prior         = self.gibbs.gibbs_sample(self.flag_initialize,\n",
    "                                                                                  z1_prior,\n",
    "                                                                                  z2_prior,\n",
    "                                                                                  self.g11,\n",
    "                                                                                  g22,\n",
    "                                                                                  torch.zeros_like(mu1),\n",
    "                                                                                  torch.zeros_like(var1),\n",
    "                                                                                  torch.zeros_like(mu2),\n",
    "                                                                                  torch.zeros_like(var2),\n",
    "                                                                                  n_iterations=5)\n",
    "        self.z1_gibbs_posterior,self.z2_gibbs_posterior = self.gibbs.gibbs_sample(self.flag_initialize,\n",
    "                                                                                  z1_posterior,\n",
    "                                                                                  z2_posterior,\n",
    "                                                                                  self.g11,\n",
    "                                                                                  g22,\n",
    "                                                                                  mu1,\n",
    "                                                                                  var1,\n",
    "                                                                                  mu2,\n",
    "                                                                                  var2,\n",
    "                                                                                  n_iterations=5)\n",
    "        self.z1_posterior = self.z1_gibbs_posterior.detach()\n",
    "        self.z2_posterior = self.z2_gibbs_posterior.detach()\n",
    "        self.z1_prior = self.z1_gibbs_prior.detach()\n",
    "        self.z2_prior = self.z2_gibbs_prior.detach()\n",
    "        G1 = torch.cat((self.g11,self.g12),0)\n",
    "        G2 = torch.cat((self.g12,g22),0)\n",
    "        G  = torch.cat((G1,G2),1)\n",
    "\n",
    "        # decoding for MNIST1\n",
    "        x1 = F.relu(self.set1_dec1(self.z1_gibbs_posterior))\n",
    "        x1 = self.set1_dec2(x1)\n",
    "        # decoding for MNIST2\n",
    "        x2 = F.relu(self.set2_dec1(self.z2_gibbs_posterior))\n",
    "        x2 = self.set2_dec2(x2)\n",
    "        \n",
    "        \n",
    "        if self.use_mse_loss:\n",
    "            reconstruction1 = self.set1_dec3(x1)\n",
    "            reconstruction2 = self.set2_dec3(x2)\n",
    "        else:\n",
    "            reconstruction1 = torch.sigmoid(self.set1_dec3(x1))\n",
    "            reconstruction2 = torch.sigmoid(self.set2_dec3(x2))\n",
    "        \n",
    "        \n",
    "        # calculating loss\n",
    "        part_fun0,part_fun1,part_fun2 = self.kl_div.calc(G,self.z1_gibbs_posterior,self.z2_gibbs_posterior,self.z1_gibbs_prior,self.z2_gibbs_prior,mu1,var1,mu2,var2)\n",
    "        if self.use_mse_loss:\n",
    "            mse_loss = nn.MSELoss(reduction='sum')\n",
    "            MSE1 = mse_loss(reconstruction1, data1)\n",
    "            MSE2 = mse_loss(reconstruction2, data2)\n",
    "        else:\n",
    "            bce_loss = nn.BCELoss(reduction='sum')\n",
    "            MSE1 = bce_loss(reconstruction1, data1)\n",
    "            MSE2 = bce_loss(reconstruction2, data2)\n",
    "\n",
    "        KLD  = part_fun0+part_fun1+part_fun2\n",
    "        if self.flag_initialize==0:\n",
    "#             make_dot(g22,params=dict(model.named_parameters()),show_attrs=True, show_saved=True).render(\"g22\", format=\"png\")\n",
    "#             make_dot(self.g11,params=dict(model.named_parameters()),show_attrs=True, show_saved=True).render(\"g11\", format=\"png\")\n",
    "\n",
    "#             make_dot(part_fun1,params=dict(model.named_parameters()),show_attrs=True, show_saved=True).render(\"part_fun1\", format=\"png\")\n",
    "#             make_dot(part_fun2,params=dict(model.named_parameters()),show_attrs=True, show_saved=True).render(\"part_fun2\", format=\"png\")\n",
    "#             make_dot(MSE1,params=dict(model.named_parameters()),show_attrs=True, show_saved=True).render(\"MSE1\", format=\"png\")\n",
    "#             make_dot(MSE2,params=dict(model.named_parameters()),show_attrs=True, show_saved=True).render(\"MSE2\", format=\"png\")\n",
    "#             make_dot(self.z1_gibbs_prior,params=dict(model.named_parameters()),show_attrs=True, show_saved=True).render(\"z1_prior\", format=\"png\")\n",
    "#             make_dot(self.z2_gibbs_prior,params=dict(model.named_parameters()),show_attrs=True, show_saved=True).render(\"z2_prior\", format=\"png\")\n",
    "#             make_dot(self.z1_gibbs_posterior,params=dict(model.named_parameters()),show_attrs=True, show_saved=True).render(\"z1_posterior\", format=\"png\")\n",
    "#             make_dot(self.z2_gibbs_posterior,params=dict(model.named_parameters()),show_attrs=True, show_saved=True).render(\"z2_posterior\", format=\"png\")\n",
    "#             make_dot(self.z2_posterior,params=dict(model.named_parameters()),show_attrs=True, show_saved=True).render(\"z2_posterior_val\", format=\"png\")\n",
    "#             make_dot(mu1,params=dict(model.named_parameters()),show_attrs=True, show_saved=True).render(\"mu1\", format=\"png\")\n",
    "#             make_dot(var1,params=dict(model.named_parameters()),show_attrs=True, show_saved=True).render(\"var1\", format=\"png\")\n",
    "#             make_dot(mu2,params=dict(model.named_parameters()),show_attrs=True, show_saved=True).render(\"mu2\", format=\"png\")\n",
    "#             make_dot(var2,params=dict(model.named_parameters()),show_attrs=True, show_saved=True).render(\"var2\", format=\"png\")\n",
    "\n",
    "            self.flag_initialize=5\n",
    "        \n",
    "        loss = MSE1+MSE2+KLD\n",
    "\n",
    "        return reconstruction1,reconstruction2,mu1,var1,mu2,var2,loss, MSE1, MSE2, KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "boolean-starter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set1_enc1.weight\n",
      "set1_enc1.bias\n",
      "set1_enc2.weight\n",
      "set1_enc2.bias\n",
      "set1_enc3.weight\n",
      "set1_enc3.bias\n",
      "set2_enc1.weight\n",
      "set2_enc1.bias\n",
      "set2_enc2.weight\n",
      "set2_enc2.bias\n",
      "set2_enc3.weight\n",
      "set2_enc3.bias\n",
      "set1_dec1.weight\n",
      "set1_dec1.bias\n",
      "set1_dec2.weight\n",
      "set1_dec2.bias\n",
      "set1_dec3.weight\n",
      "set1_dec3.bias\n",
      "set2_dec1.weight\n",
      "set2_dec1.bias\n",
      "set2_dec2.weight\n",
      "set2_dec2.bias\n",
      "set2_dec3.weight\n",
      "set2_dec3.bias\n"
     ]
    }
   ],
   "source": [
    "model = VAE(latent_dim1, latent_dim2, batch_size,use_mse_loss=True).to(device)\n",
    "optimizer = optim.Adam(model.parameters(),lr=lr)\n",
    "for name, para in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "thick-grounds",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,joint_dataloader,epoch):\n",
    "    model.train()\n",
    "    running_mse1 = 0.0\n",
    "    running_mse2 = 0.0\n",
    "    running_kld  = 0.0\n",
    "    running_loss = 0.0\n",
    "    for i,joint_data in enumerate(joint_dataloader):\n",
    "        data1    = joint_data[0]\n",
    "        data1    = data1.float()\n",
    "        data2    = joint_data[1]\n",
    "        data2    = data2.float()\n",
    "        data1    = data1.to(device)\n",
    "        data2    = data2.to(device)\n",
    "        data1    = data1.view(data1.size(0), -1)\n",
    "        data2    = data2.view(data2.size(0), -1)\n",
    "        optimizer.zero_grad()\n",
    "        reconstruction1,reconstruction2,mu1,var1,mu2,var2,loss, MSE1, MSE2, KLD       = model(data1,data2) \n",
    "        running_mse1 += MSE1.item()\n",
    "        running_mse2 += MSE2.item()\n",
    "        running_kld  += KLD.item()\n",
    "        running_loss += loss.item()          #.item converts tensor with one element to number\n",
    "        loss.backward()                      #.backward\n",
    "        optimizer.step()                     #.step one learning step\n",
    "#         for name, para in model.named_parameters():\n",
    "#             print(torch.sum(para.grad))\n",
    "    train_loss = running_loss/(len(joint_dataloader.dataset))\n",
    "    mse1_loss = running_mse1 / (len(joint_dataloader.dataset))\n",
    "    mse2_loss = running_mse2 / (len(joint_dataloader.dataset))\n",
    "    kld_loss = running_kld / (len(joint_dataloader.dataset))\n",
    "#     # Log this every 100 epochs\n",
    "#     if epoch % 100 == 0:\n",
    "    for name, param in model.named_parameters():\n",
    "        writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
    "    writer.add_scalar(\"training/loss\", train_loss, epoch)\n",
    "    writer.add_scalar(\"training/MSE1\", mse1_loss, epoch)\n",
    "    writer.add_scalar(\"training/MSE2\", mse2_loss, epoch)\n",
    "    writer.add_scalar(\"training/KLD\", kld_loss, epoch)    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return train_loss\n",
    "    \n",
    "def test(model,joint_dataloader,epoch):\n",
    "\n",
    "    model.eval()\n",
    "    running_mse1 = 0.0\n",
    "    running_mse2 = 0.0\n",
    "    running_kld  = 0.0\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i,joint_data in enumerate(joint_dataloader):\n",
    "            data1   = joint_data[0]\n",
    "            data1   = data1.float()\n",
    "            data2   = joint_data[1]\n",
    "            data2   = data2.float()\n",
    "            data1   = data1.to(device)\n",
    "            data2   = data2.to(device)\n",
    "            data1   = data1.view(data1.size(0), -1)\n",
    "            data2   = data2.view(data2.size(0), -1)\n",
    "            reconstruction1,reconstruction2,mu1,var1,mu2,var2,loss, MSE1, MSE2, KLD = model(data1,data2)  \n",
    "            running_loss += loss.item()\n",
    "            running_mse1 += MSE1.item()\n",
    "            running_mse2 += MSE2.item()\n",
    "            running_kld += KLD.item()\n",
    "            #save the last batch input and output of every epoch\n",
    "            if i == int(len(joint_dataloader.dataset)/joint_dataloader.batch_size) - 1:\n",
    "                num_rows = 8\n",
    "                both  = torch.cat((data1.view(batch_size, 1, 28, 28)[:8], \n",
    "                                  reconstruction1.view(batch_size, 1, 28, 28)[:8]))\n",
    "                bothp = torch.cat((data2.view(batch_size, 1, 28, 28)[:8], \n",
    "                                  reconstruction2.view(batch_size, 1, 28, 28)[:8]))\n",
    "                save_image(both.cpu(),  os.path.join(RECONSTRUCTION_PATH, f\"1_outputMNIST_1_{epoch}.png\"), nrow=num_rows)\n",
    "                save_image(bothp.cpu(), os.path.join(RECONSTRUCTION_PATH, f\"2_outputMNIST_2_{epoch}.png\"), nrow=num_rows)\n",
    "\n",
    "    test_loss = running_loss/(len(joint_dataloader.dataset))\n",
    "    mse1_loss = running_mse1 / (len(joint_dataloader.dataset))\n",
    "    mse2_loss = running_mse2 / (len(joint_dataloader.dataset))\n",
    "    kld_loss = running_kld / (len(joint_dataloader.dataset))\n",
    "    writer.add_scalar(\"validation/loss\", test_loss, epoch)\n",
    "    writer.add_scalar(\"validation/MSE1\", mse1_loss, epoch)\n",
    "    writer.add_scalar(\"validation/MSE2\", mse2_loss, epoch)\n",
    "    writer.add_scalar(\"validation/KLD\", kld_loss, epoch)\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "pointed-bicycle",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 100\n",
      "Train Loss: 10906413.3125\n",
      "Test Loss: 9123196.0625\n",
      "Epoch 2 of 100\n",
      "Train Loss: 10879946.4375\n",
      "Test Loss: 9089757.0000\n",
      "Epoch 3 of 100\n",
      "Train Loss: 10778051.2500\n",
      "Test Loss: 8984329.8438\n",
      "Epoch 4 of 100\n",
      "Train Loss: 10498517.0000\n",
      "Test Loss: 8809368.1562\n",
      "Epoch 5 of 100\n",
      "Train Loss: 10042588.1250\n",
      "Test Loss: 8500410.1250\n",
      "Epoch 6 of 100\n",
      "Train Loss: 9375219.7500\n",
      "Test Loss: 8039398.9375\n",
      "Epoch 7 of 100\n",
      "Train Loss: 8514234.4375\n",
      "Test Loss: 7681919.5938\n",
      "Epoch 8 of 100\n",
      "Train Loss: 7945919.8125\n",
      "Test Loss: 7473390.5625\n",
      "Epoch 9 of 100\n",
      "Train Loss: 7310316.0000\n",
      "Test Loss: 6959777.1875\n",
      "Epoch 10 of 100\n",
      "Train Loss: 6323494.8125\n",
      "Test Loss: 6482844.9688\n",
      "Epoch 11 of 100\n",
      "Train Loss: 5656601.6875\n",
      "Test Loss: 6478638.0000\n",
      "Epoch 12 of 100\n",
      "Train Loss: 5552728.8750\n",
      "Test Loss: 6504078.2500\n",
      "Epoch 13 of 100\n",
      "Train Loss: 5458884.4375\n",
      "Test Loss: 6495965.4062\n",
      "Epoch 14 of 100\n",
      "Train Loss: 5410007.2188\n",
      "Test Loss: 6504445.2500\n",
      "Epoch 15 of 100\n",
      "Train Loss: 5386901.0000\n",
      "Test Loss: 6516019.0625\n",
      "Epoch 16 of 100\n",
      "Train Loss: 5381751.8438\n",
      "Test Loss: 6513704.4062\n",
      "Epoch 17 of 100\n",
      "Train Loss: 5378788.5312\n",
      "Test Loss: 6519605.4375\n",
      "Epoch 18 of 100\n",
      "Train Loss: 5359347.1875\n",
      "Test Loss: 6508691.5625\n",
      "Epoch 19 of 100\n",
      "Train Loss: 5360695.1250\n",
      "Test Loss: 6498681.1562\n",
      "Epoch 20 of 100\n",
      "Train Loss: 5354099.0625\n",
      "Test Loss: 6496165.1250\n",
      "Epoch 21 of 100\n",
      "Train Loss: 5376253.1875\n",
      "Test Loss: 6513697.0312\n",
      "Epoch 22 of 100\n",
      "Train Loss: 5354691.0312\n",
      "Test Loss: 6520269.0625\n",
      "Epoch 23 of 100\n",
      "Train Loss: 5360305.6250\n",
      "Test Loss: 6526328.3750\n",
      "Epoch 24 of 100\n",
      "Train Loss: 5357523.4375\n",
      "Test Loss: 6521168.6875\n",
      "Epoch 25 of 100\n",
      "Train Loss: 5349564.3750\n",
      "Test Loss: 6518030.2500\n",
      "Epoch 26 of 100\n",
      "Train Loss: 5361859.6875\n",
      "Test Loss: 6516347.8125\n",
      "Epoch 27 of 100\n",
      "Train Loss: 5351569.3750\n",
      "Test Loss: 6513872.9688\n",
      "Epoch 28 of 100\n",
      "Train Loss: 5355292.0938\n",
      "Test Loss: 6522074.0938\n",
      "Epoch 29 of 100\n",
      "Train Loss: 5351705.2812\n",
      "Test Loss: 6517384.5312\n",
      "Epoch 30 of 100\n",
      "Train Loss: 5347030.8750\n",
      "Test Loss: 6524295.1562\n",
      "Epoch 31 of 100\n",
      "Train Loss: 5349574.6250\n",
      "Test Loss: 6532269.8125\n",
      "Epoch 32 of 100\n",
      "Train Loss: 5357348.5625\n",
      "Test Loss: 6523006.3438\n",
      "Epoch 33 of 100\n",
      "Train Loss: 5354651.7188\n",
      "Test Loss: 6519180.8125\n",
      "Epoch 34 of 100\n",
      "Train Loss: 5353494.5000\n",
      "Test Loss: 6507799.3750\n",
      "Epoch 35 of 100\n",
      "Train Loss: 5357652.4375\n",
      "Test Loss: 6517864.5625\n",
      "Epoch 36 of 100\n",
      "Train Loss: 5352948.0000\n",
      "Test Loss: 6506592.6875\n",
      "Epoch 37 of 100\n",
      "Train Loss: 5354741.0625\n",
      "Test Loss: 6516208.1250\n",
      "Epoch 38 of 100\n",
      "Train Loss: 5347884.7188\n",
      "Test Loss: 6516856.4375\n",
      "Epoch 39 of 100\n",
      "Train Loss: 5351530.2500\n",
      "Test Loss: 6527657.6562\n",
      "Epoch 40 of 100\n",
      "Train Loss: 5360564.2188\n",
      "Test Loss: 6509369.1250\n",
      "Epoch 41 of 100\n",
      "Train Loss: 5344966.4375\n",
      "Test Loss: 6512239.4375\n",
      "Epoch 42 of 100\n",
      "Train Loss: 5348181.7500\n",
      "Test Loss: 6503411.8125\n",
      "Epoch 43 of 100\n",
      "Train Loss: 5344850.4062\n",
      "Test Loss: 6502767.3125\n",
      "Epoch 44 of 100\n",
      "Train Loss: 5353592.6875\n",
      "Test Loss: 6513725.6875\n",
      "Epoch 45 of 100\n",
      "Train Loss: 5355476.3750\n",
      "Test Loss: 6524893.9375\n",
      "Epoch 46 of 100\n",
      "Train Loss: 5346218.5625\n",
      "Test Loss: 6516459.7500\n",
      "Epoch 47 of 100\n",
      "Train Loss: 5346521.9375\n",
      "Test Loss: 6516889.7500\n",
      "Epoch 48 of 100\n",
      "Train Loss: 5350231.1875\n",
      "Test Loss: 6522457.3125\n",
      "Epoch 49 of 100\n",
      "Train Loss: 5349904.0312\n",
      "Test Loss: 6527935.6875\n",
      "Epoch 50 of 100\n",
      "Train Loss: 5353428.8125\n",
      "Test Loss: 6523643.3125\n",
      "Epoch 51 of 100\n",
      "Train Loss: 5347519.1562\n",
      "Test Loss: 6525303.1250\n",
      "Epoch 52 of 100\n",
      "Train Loss: 5344814.1562\n",
      "Test Loss: 6525499.3750\n",
      "Epoch 53 of 100\n",
      "Train Loss: 5355491.5000\n",
      "Test Loss: 6520013.4375\n",
      "Epoch 54 of 100\n",
      "Train Loss: 5355505.8750\n",
      "Test Loss: 6525680.9062\n",
      "Epoch 55 of 100\n",
      "Train Loss: 5349659.6875\n",
      "Test Loss: 6516327.7812\n",
      "Epoch 56 of 100\n",
      "Train Loss: 5345332.3438\n",
      "Test Loss: 6509977.4375\n",
      "Epoch 57 of 100\n",
      "Train Loss: 5350084.9688\n",
      "Test Loss: 6512861.2500\n",
      "Epoch 58 of 100\n",
      "Train Loss: 5349110.1562\n",
      "Test Loss: 6516998.3750\n",
      "Epoch 59 of 100\n",
      "Train Loss: 5358067.5000\n",
      "Test Loss: 6535074.2812\n",
      "Epoch 60 of 100\n",
      "Train Loss: 5340778.6562\n",
      "Test Loss: 6528492.1875\n",
      "Epoch 61 of 100\n",
      "Train Loss: 5347020.7500\n",
      "Test Loss: 6525175.6250\n",
      "Epoch 62 of 100\n",
      "Train Loss: 5360664.5625\n",
      "Test Loss: 6518726.5625\n",
      "Epoch 63 of 100\n",
      "Train Loss: 5350659.8750\n",
      "Test Loss: 6515624.8750\n",
      "Epoch 64 of 100\n",
      "Train Loss: 5353724.3125\n",
      "Test Loss: 6513155.9375\n",
      "Epoch 65 of 100\n",
      "Train Loss: 5351306.2500\n",
      "Test Loss: 6516832.3750\n",
      "Epoch 66 of 100\n",
      "Train Loss: 5353735.5000\n",
      "Test Loss: 6520344.1875\n",
      "Epoch 67 of 100\n",
      "Train Loss: 5355040.3125\n",
      "Test Loss: 6523335.3125\n",
      "Epoch 68 of 100\n",
      "Train Loss: 5343669.6562\n",
      "Test Loss: 6518244.6875\n",
      "Epoch 69 of 100\n",
      "Train Loss: 5347646.8125\n",
      "Test Loss: 6524134.8125\n",
      "Epoch 70 of 100\n",
      "Train Loss: 5353639.5000\n",
      "Test Loss: 6515531.8125\n",
      "Epoch 71 of 100\n",
      "Train Loss: 5345974.5938\n",
      "Test Loss: 6519226.5000\n",
      "Epoch 72 of 100\n",
      "Train Loss: 5347519.8438\n",
      "Test Loss: 6529452.3125\n",
      "Epoch 73 of 100\n",
      "Train Loss: 5349615.6250\n",
      "Test Loss: 6520505.1250\n",
      "Epoch 74 of 100\n",
      "Train Loss: 5348071.6562\n",
      "Test Loss: 6521050.0000\n",
      "Epoch 75 of 100\n",
      "Train Loss: 5347298.6875\n",
      "Test Loss: 6514227.8750\n",
      "Epoch 76 of 100\n",
      "Train Loss: 5360736.5938\n",
      "Test Loss: 6506624.4375\n",
      "Epoch 77 of 100\n",
      "Train Loss: 5344353.3750\n",
      "Test Loss: 6514305.3125\n",
      "Epoch 78 of 100\n",
      "Train Loss: 5351094.2500\n",
      "Test Loss: 6525232.1250\n",
      "Epoch 79 of 100\n",
      "Train Loss: 5342877.3750\n",
      "Test Loss: 6514294.3125\n",
      "Epoch 80 of 100\n",
      "Train Loss: 5346896.1875\n",
      "Test Loss: 6528037.3750\n",
      "Epoch 81 of 100\n",
      "Train Loss: 5340501.6875\n",
      "Test Loss: 6520383.6875\n",
      "Epoch 82 of 100\n",
      "Train Loss: 5341852.5000\n",
      "Test Loss: 6517848.0625\n",
      "Epoch 83 of 100\n",
      "Train Loss: 5342441.3750\n",
      "Test Loss: 6514313.7500\n",
      "Epoch 84 of 100\n",
      "Train Loss: 5343246.1250\n",
      "Test Loss: 6518819.0625\n",
      "Epoch 85 of 100\n",
      "Train Loss: 5355463.2812\n",
      "Test Loss: 6524999.7500\n",
      "Epoch 86 of 100\n",
      "Train Loss: 5348993.3125\n",
      "Test Loss: 6520376.6250\n",
      "Epoch 87 of 100\n",
      "Train Loss: 5349719.8750\n",
      "Test Loss: 6532893.3438\n",
      "Epoch 88 of 100\n",
      "Train Loss: 5343720.3750\n",
      "Test Loss: 6528639.1875\n",
      "Epoch 89 of 100\n",
      "Train Loss: 5339862.8125\n",
      "Test Loss: 6527501.5625\n",
      "Epoch 90 of 100\n",
      "Train Loss: 5353362.2188\n",
      "Test Loss: 6518971.9375\n",
      "Epoch 91 of 100\n",
      "Train Loss: 5340854.9375\n",
      "Test Loss: 6521430.4688\n",
      "Epoch 92 of 100\n",
      "Train Loss: 5346016.9375\n",
      "Test Loss: 6520739.8750\n",
      "Epoch 93 of 100\n",
      "Train Loss: 5340443.1250\n",
      "Test Loss: 6516979.8750\n",
      "Epoch 94 of 100\n",
      "Train Loss: 5353283.2188\n",
      "Test Loss: 6523189.7500\n",
      "Epoch 95 of 100\n",
      "Train Loss: 5354050.8750\n",
      "Test Loss: 6509641.1875\n",
      "Epoch 96 of 100\n",
      "Train Loss: 5354332.8125\n",
      "Test Loss: 6530451.0312\n",
      "Epoch 97 of 100\n",
      "Train Loss: 5348879.3125\n",
      "Test Loss: 6515659.8750\n",
      "Epoch 98 of 100\n",
      "Train Loss: 5349017.3750\n",
      "Test Loss: 6528438.2812\n",
      "Epoch 99 of 100\n",
      "Train Loss: 5347466.3125\n",
      "Test Loss: 6521953.5625\n",
      "Epoch 100 of 100\n",
      "Train Loss: 5349609.3750\n",
      "Test Loss: 6522370.2500\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "test_loss = []\n",
    "epochs = 100\n",
    "writer=SummaryWriter(SUMMARY_WRITER_PATH)\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1} of {epochs}\")\n",
    "    train_epoch_loss = train(model,joint_dataset_train_loader,epoch)\n",
    "    test_epoch_loss  = test(model,joint_dataset_test_loader,epoch)\n",
    "    train_loss.append(train_epoch_loss)\n",
    "    test_loss.append(test_epoch_loss)     \n",
    "    print(f\"Train Loss: {train_epoch_loss:.4f}\")\n",
    "    print(f\"Test Loss: {test_epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "conceptual-perfume",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set1_enc1.weight\n",
      "set1_enc1.bias\n",
      "set1_enc2.weight\n",
      "set1_enc2.bias\n",
      "set1_enc3.weight\n",
      "set1_enc3.bias\n",
      "set2_enc1.weight\n",
      "set2_enc1.bias\n",
      "set2_enc2.weight\n",
      "set2_enc2.bias\n",
      "set2_enc3.weight\n",
      "set2_enc3.bias\n",
      "set1_dec1.weight\n",
      "set1_dec1.bias\n",
      "set1_dec2.weight\n",
      "set1_dec2.bias\n",
      "set1_dec3.weight\n",
      "set1_dec3.bias\n",
      "set2_dec1.weight\n",
      "set2_dec1.bias\n",
      "set2_dec2.weight\n",
      "set2_dec2.bias\n",
      "set2_dec3.weight\n",
      "set2_dec3.bias\n"
     ]
    }
   ],
   "source": [
    "for name, para in model.named_parameters():\n",
    "            print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulated-navigator",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "achint-env2",
   "language": "python",
   "name": "achint-env2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
